{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "709a279d",
   "metadata": {},
   "source": [
    "## Pseudo-labing\n",
    "\n",
    "The goal of this python file is to get a dataset that is a merge of both the two datasets that we have and use it to train our model.\n",
    "\n",
    "We have:\n",
    "* Dataset A → images + age + gender + race\n",
    "* Dataset B → images + emotion only\n",
    "\n",
    "We want to:\n",
    "1. Train a demographics model on Dataset A\n",
    "2. Use it to predict age/gender/race on Dataset B\n",
    "3. Save those predictions as pseudo-labels with confidence\n",
    "4. Merge the datasets safely\n",
    "\n",
    "This is called **pseudo-labeling**\n",
    "\n",
    "Important: these are not true labels, so we store them separately and track confidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba82e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/renubandaru/Code/GitHub/facial-profiler/deeplearning/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48081a",
   "metadata": {},
   "source": [
    "### STEP 1 -Load Dataset A and Dataset B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cba62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_path  age  gender  Race\n",
      "0  source_data/UTK-Face/part3/27_0_1_201701201338...   27       0     1\n",
      "1  source_data/UTK-Face/part3/24_0_3_201701191655...   24       0     3\n",
      "2  source_data/UTK-Face/part3/8_1_0_2017011715460...    8       1     0\n",
      "3  source_data/UTK-Face/part3/85_1_0_201701202226...   85       1     0\n",
      "4  source_data/UTK-Face/part3/26_1_0_201701191929...   26       1     0\n",
      "                                          image_path  emotion\n",
      "0  source_data/raf/DATASET/train/7/train_11651_al...        7\n",
      "1  source_data/raf/DATASET/train/7/train_10043_al...        7\n",
      "2  source_data/raf/DATASET/train/7/train_11301_al...        7\n",
      "3  source_data/raf/DATASET/train/7/train_10513_al...        7\n",
      "4  source_data/raf/DATASET/train/7/train_11148_al...        7\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "dfA = pd.read_csv(\"utk_face_labels.csv\")   # age, gender, race\n",
    "dfB = pd.read_csv(\"raf_labels.csv\")   # emotion only\n",
    "\n",
    "print(dfA.head())\n",
    "print(dfB.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e6ec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dfA shape: (24102, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: bad Huffman code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dfA shape: (24102, 4)\n"
     ]
    }
   ],
   "source": [
    "#STEP 1.5 — Validate and Clean Image Paths\n",
    "\n",
    "def validate_image_paths(df):\n",
    "    \"\"\"Remove rows with corrupt or unreadable images.\"\"\"\n",
    "    valid_paths = []\n",
    "    for path in df['image_path']:\n",
    "        try:\n",
    "            img = cv2.imread(path)\n",
    "            if img is not None and img.size > 0:\n",
    "                valid_paths.append(path)\n",
    "        except:\n",
    "            continue\n",
    "    return df[df['image_path'].isin(valid_paths)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original dfA shape: {dfA.shape}\")\n",
    "dfA = validate_image_paths(dfA)\n",
    "print(f\"Cleaned dfA shape: {dfA.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ebffbd",
   "metadata": {},
   "source": [
    "### STEP 2 — Convert Age to Bins\n",
    "\n",
    "We are doing this step because the exact age prediction is noisy and difficult. Age classificiation into bins is more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc2e748a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique age bins in dfA: [2 0 6 5 3 4 1]\n",
      "   age  age_bin\n",
      "0   27        2\n",
      "1   24        2\n",
      "2    8        0\n",
      "3   85        6\n",
      "4   26        2\n",
      "5   57        5\n",
      "6   33        3\n",
      "7   78        6\n",
      "8   45        4\n",
      "9   34        3\n"
     ]
    }
   ],
   "source": [
    "# Create age bins\n",
    "age_bins = [0,10,20,30,40,50,60,200]\n",
    "\n",
    "def age_to_bin(age):\n",
    "    return np.digitize(age, age_bins) - 1\n",
    "\n",
    "# Convert age to bins and add as a new column in dfA\n",
    "dfA[\"age_bin\"] = dfA[\"age\"].apply(age_to_bin)\n",
    "\n",
    "# printing the unique age bins to verify\n",
    "print(\"Unique age bins in dfA:\", dfA[\"age_bin\"].unique())\n",
    "\n",
    "print(dfA[[\"age\", \"age_bin\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e02c59",
   "metadata": {},
   "source": [
    "### STEP 3 — Converting Gender to Integers\n",
    "\n",
    "Neural networks need numeric labels. Therefore we need to convert them accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d4191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original dataset shape: (24102, 5)\n",
      "Dataset shape after dropping gender=3: (24102, 5)\n",
      "Number of age bins: 7\n",
      "Number of gender classes: 2\n"
     ]
    }
   ],
   "source": [
    "# Drop rows where gender class is 3 since its meaning is unclear and it may be an outlier or error in the dataset\n",
    "print(f\"\\nOriginal dataset shape: {dfA.shape}\")\n",
    "dfA = dfA[dfA['gender'] != 3]\n",
    "print(f\"Dataset shape after dropping gender=3: {dfA.shape}\")\n",
    "\n",
    "# Convert gender to categorical if it's not already numeric\n",
    "if dfA[\"gender\"].dtype == \"object\":\n",
    "    dfA[\"gender\"] = dfA[\"gender\"].astype(\"category\").cat.codes\n",
    "\n",
    "num_age = dfA[\"age_bin\"].nunique()\n",
    "num_gender = dfA[\"gender\"].nunique()\n",
    "\n",
    "print(f\"Number of age bins: {num_age}\")\n",
    "print(f\"Number of gender classes: {num_gender}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacaa9c3",
   "metadata": {},
   "source": [
    "### STEP 4 — Train/Validation Split\n",
    "\n",
    "Both shuffle=True and stratify=dfA[\"age_bin\"] are important for proper model training and evaluation. \n",
    "\n",
    "* Benefits of shuffle=True:\n",
    "    - Prevents order bias: Without shuffling, if your data is sorted by age, the model might learn patterns based on the order rather than actual features\n",
    "    - Better generalization: Random mixing ensures the model sees diverse examples in each batch\n",
    "    - Prevents overfitting to data patterns: Shuffling breaks any inherent ordering that might exist in your dataset\n",
    "\n",
    "* Benefits of stratify=dfA[\"age_bin\"]:\n",
    "    - Balanced age distribution: Ensures both training and validation sets have the same proportion of each age group\n",
    "    - Prevents bias: Without stratification, some age groups might be underrepresented in validation, leading to unreliable performance metrics\n",
    "    - More accurate evaluation: Your validation set will better represent the real-world age distribution\n",
    "    - Stable training: Prevents scenarios where certain age groups are only in training or only in validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ff01b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation split for dataset A with stratification on age bins\n",
    "trainA, valA = train_test_split(\n",
    "    dfA,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=dfA[\"age_bin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e2e07",
   "metadata": {},
   "source": [
    " ### STEP 5 — Create TensorFlow Data Pipelin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ffc9de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224  # Define a consistent image size for the model\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def preprocess_image(path):\n",
    "    try:\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = img / 255.0\n",
    "        return img\n",
    "    except:\n",
    "        # Return blank/gray image on error\n",
    "        return tf.zeros((IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "def create_datasetA(df, training=True):\n",
    "    image_paths = df[\"image_path\"].values\n",
    "    age = df[\"age_bin\"].values\n",
    "    gender = df[\"gender\"].values\n",
    "    \n",
    "    # Create a TensorFlow dataset from the image paths and labels\n",
    "    ds = tf.data.Dataset.from_tensor_slices((image_paths, age, gender))\n",
    "\n",
    "    # Map the dataset to load and preprocess images and return labels\n",
    "    def load_data(path, age, gender):\n",
    "        img = preprocess_image(path)\n",
    "        # Apply stronger augmentation only during training to improve generalization\n",
    "        if training:\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tf.image.random_crop(img, size=[IMG_SIZE, IMG_SIZE, 3])\n",
    "            img = tf.image.random_brightness(img, max_delta=0.1)\n",
    "            img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
    "            img = tf.image.random_saturation(img, lower=0.9, upper=1.1)\n",
    "        return img, {\"age\": age, \"gender\": gender}\n",
    "    \n",
    "    # Map the dataset to load and preprocess images and return labels\n",
    "    ds = ds.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_ds = create_datasetA(trainA, training=True)\n",
    "val_ds = create_datasetA(valA, training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb7b50",
   "metadata": {},
   "source": [
    "### STEP 6 — Build Multi-Output Model\n",
    "\n",
    "This is a very important step\n",
    "\n",
    "We use:\n",
    "- Pretrained MobileNetV2\n",
    "- 2 output heads:\n",
    "    - age\n",
    "    - gender\n",
    "Shared feature extractor → multiple tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c12d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model - MobileNetV2 as the base model for feature extraction\n",
    "base_model = keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    ")\n",
    "\n",
    "base_model.trainable = False  # Freeze the base model first\n",
    "\n",
    "# Add custom layers on top of the base model for age and gender\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Separate processing for age head (more capacity)\n",
    "x_age = layers.Dense(512, activation='relu')(x)\n",
    "x_age = layers.Dropout(0.4)(x_age)\n",
    "age_output = layers.Dense(num_age, activation='softmax', name='age')(x_age)\n",
    "\n",
    "# Gender head (keep simple)\n",
    "gender_output = layers.Dense(num_gender, activation='softmax', name='gender')(x)\n",
    "\n",
    "# Output layers for age and gender\n",
    "age_output = layers.Dense(num_age, activation=\"softmax\", kernel_regularizer=keras.regularizers.l2(0.001), name=\"age\")(x) # adding L2 regularization to prevent overfitting\n",
    "gender_output = layers.Dense(num_gender, activation=\"softmax\", name=\"gender\")(x)\n",
    "\n",
    "\n",
    "# Create the model with two outputs\n",
    "model = keras.Model(inputs=inputs, outputs=[age_output, gender_output])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059582a1",
   "metadata": {},
   "source": [
    "### STEP 7 — Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e17204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, 'int32')\n",
    "        y_true = K.one_hot(y_true, K.shape(y_pred)[-1])\n",
    "        p_t = K.sum(y_true * y_pred, axis=-1)\n",
    "        loss = -alpha * K.pow(1 - p_t, gamma) * K.log(K.clip(p_t, 1e-7, 1.0))\n",
    "        return loss\n",
    "    return loss\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss={\n",
    "        \"age\": focal_loss(gamma=2., alpha=.25), # focal loss for imbalanced age bins\n",
    "\n",
    "        \"gender\": keras.losses.SparseCategoricalCrossentropy()\n",
    "    },\n",
    "    loss_weights={\"age\": 2.0, \"gender\": 1.0},  # ← Emphasize age\n",
    "    metrics={\n",
    "        \"age\": \"accuracy\",\n",
    "        \"gender\": \"accuracy\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55902e3",
   "metadata": {},
   "source": [
    "### STEP 8 — Train on Dataset A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5df58e",
   "metadata": {},
   "source": [
    "**Why Two-Phase (Frozen → Fine-tune) is Better Than Direct Fine-tuning**\n",
    "\n",
    "The Problem with Starting Unfrozen:\n",
    "* MobileNetV2 pretrained on ImageNet already has good generic features (edges, textures, shapes)\n",
    "* If you unfreeze immediately with high learning rate, you overwrite these good features with poor updates (since your face dataset is small initially)\n",
    "* Result: Catastrophic forgetting — model loses useful pretrained knowledge\n",
    "\n",
    "**Why Two-Phase Works:**\n",
    "1. Phase 1 (Frozen, 5 epochs):\n",
    "    - Backbone weights locked → can't change\n",
    "    - Only age/gender heads learn to adapt pretrained features to your task\n",
    "    - Stable & fast — reuses pretrained knowledge effectively\n",
    "\n",
    "2. Phase 2 (Unfrozen, 15 epochs, LR=1e-5):\n",
    "\n",
    "    - After the age head has learned, unfreeze last 20 backbone layers\n",
    "    - Use a very low learning rate (1e-5) → slow, small updates only\n",
    "    - Gently fine-tune backbone features to specialize for age/gender\n",
    "    - Backbone is already initialized with good features, so small updates improve accuracy\n",
    "\n",
    "Analogy:\n",
    "* Frozen phase: Transfer pre-trained knowledge to your task (quick win)\n",
    "* Fine-tune phase: Customize pretrained knowledge (gradual improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b11284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:19\u001b[0m 396ms/step - age_accuracy: 0.1460 - age_loss: 0.4633 - gender_accuracy: 0.3830 - gender_loss: 1.0149 - loss: 1.9555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:15:54.571434: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:06\u001b[0m 400ms/step - age_accuracy: 0.1475 - age_loss: 0.4583 - gender_accuracy: 0.3846 - gender_loss: 1.0070 - loss: 1.9375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m6s\u001b[0m 406ms/step - age_accuracy: 0.1797 - age_loss: 0.4159 - gender_accuracy: 0.4176 - gender_loss: 0.9370 - loss: 1.7826"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 512ms/step - age_accuracy: 0.1808 - age_loss: 0.4148 - gender_accuracy: 0.4190 - gender_loss: 0.9346 - loss: 1.7782 - val_age_accuracy: 0.2798 - val_age_loss: 0.3346 - val_gender_accuracy: 0.5513 - val_gender_loss: 0.7375 - val_loss: 1.4211 - learning_rate: 1.0000e-05\n",
      "Epoch 2/5\n",
      "\u001b[1m100/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:00\u001b[0m 360ms/step - age_accuracy: 0.2992 - age_loss: 0.3330 - gender_accuracy: 0.5775 - gender_loss: 0.7194 - loss: 1.3994"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:21:00.192568: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 306ms/step - age_accuracy: 0.2966 - age_loss: 0.3325 - gender_accuracy: 0.5763 - gender_loss: 0.7181 - loss: 1.3970"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m587/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 203ms/step - age_accuracy: 0.2998 - age_loss: 0.3256 - gender_accuracy: 0.6022 - gender_loss: 0.6867 - loss: 1.3516"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 244ms/step - age_accuracy: 0.3001 - age_loss: 0.3254 - gender_accuracy: 0.6033 - gender_loss: 0.6856 - loss: 1.3501 - val_age_accuracy: 0.3360 - val_age_loss: 0.3020 - val_gender_accuracy: 0.6999 - val_gender_loss: 0.5864 - val_loss: 1.2044 - learning_rate: 1.0000e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m100/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 175ms/step - age_accuracy: 0.3453 - age_loss: 0.3014 - gender_accuracy: 0.7190 - gender_loss: 0.5604 - loss: 1.1771"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:23:09.313849: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 175ms/step - age_accuracy: 0.3436 - age_loss: 0.3010 - gender_accuracy: 0.7161 - gender_loss: 0.5628 - loss: 1.1786"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m587/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 178ms/step - age_accuracy: 0.3489 - age_loss: 0.2956 - gender_accuracy: 0.7231 - gender_loss: 0.5533 - loss: 1.1583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 222ms/step - age_accuracy: 0.3491 - age_loss: 0.2955 - gender_accuracy: 0.7235 - gender_loss: 0.5528 - loss: 1.1575 - val_age_accuracy: 0.3767 - val_age_loss: 0.2800 - val_gender_accuracy: 0.7581 - val_gender_loss: 0.5056 - val_loss: 1.0796 - learning_rate: 1.0000e-05\n",
      "Epoch 4/5\n",
      "\u001b[1m100/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 175ms/step - age_accuracy: 0.3752 - age_loss: 0.2810 - gender_accuracy: 0.7779 - gender_loss: 0.4836 - loss: 1.0593"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:25:23.061463: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 177ms/step - age_accuracy: 0.3759 - age_loss: 0.2807 - gender_accuracy: 0.7749 - gender_loss: 0.4875 - loss: 1.0626"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m587/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 180ms/step - age_accuracy: 0.3820 - age_loss: 0.2765 - gender_accuracy: 0.7759 - gender_loss: 0.4848 - loss: 1.0517"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 224ms/step - age_accuracy: 0.3822 - age_loss: 0.2764 - gender_accuracy: 0.7761 - gender_loss: 0.4845 - loss: 1.0511 - val_age_accuracy: 0.3985 - val_age_loss: 0.2646 - val_gender_accuracy: 0.7897 - val_gender_loss: 0.4588 - val_loss: 1.0017 - learning_rate: 1.0000e-05\n",
      "Epoch 5/5\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 178ms/step - age_accuracy: 0.4092 - age_loss: 0.2631 - gender_accuracy: 0.8066 - gender_loss: 0.4290 - loss: 0.9689"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:27:38.174378: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 177ms/step - age_accuracy: 0.4079 - age_loss: 0.2630 - gender_accuracy: 0.8040 - gender_loss: 0.4345 - loss: 0.9743"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 182ms/step - age_accuracy: 0.4073 - age_loss: 0.2604 - gender_accuracy: 0.8017 - gender_loss: 0.4409 - loss: 0.9754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 226ms/step - age_accuracy: 0.4074 - age_loss: 0.2603 - gender_accuracy: 0.8017 - gender_loss: 0.4408 - loss: 0.9752 - val_age_accuracy: 0.4159 - val_age_loss: 0.2531 - val_gender_accuracy: 0.8067 - val_gender_loss: 0.4288 - val_loss: 0.9487 - learning_rate: 1.0000e-05\n",
      "Epoch 1/15\n",
      "\u001b[1m100/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 223ms/step - age_accuracy: 0.3342 - age_loss: 1.7303 - gender_accuracy: 0.7126 - gender_loss: 0.5512 - loss: 4.0255"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:30:01.870384: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 226ms/step - age_accuracy: 0.3439 - age_loss: 1.7061 - gender_accuracy: 0.7218 - gender_loss: 0.5392 - loss: 3.9651"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m587/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 227ms/step - age_accuracy: 0.4122 - age_loss: 1.5452 - gender_accuracy: 0.7735 - gender_loss: 0.4672 - loss: 3.5714"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 270ms/step - age_accuracy: 0.4136 - age_loss: 1.5416 - gender_accuracy: 0.7745 - gender_loss: 0.4657 - loss: 3.5627 - val_age_accuracy: 0.4845 - val_age_loss: 1.2948 - val_gender_accuracy: 0.8407 - val_gender_loss: 0.3638 - val_loss: 2.9668 - learning_rate: 1.0000e-05\n",
      "Epoch 2/15\n",
      "\u001b[1m 98/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:53\u001b[0m 224ms/step - age_accuracy: 0.5217 - age_loss: 1.2196 - gender_accuracy: 0.8572 - gender_loss: 0.3325 - loss: 2.7856"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:32:44.719250: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m136/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 225ms/step - age_accuracy: 0.5231 - age_loss: 1.2224 - gender_accuracy: 0.8544 - gender_loss: 0.3368 - loss: 2.7953"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m585/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 228ms/step - age_accuracy: 0.5307 - age_loss: 1.2119 - gender_accuracy: 0.8509 - gender_loss: 0.3415 - loss: 2.7790"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 274ms/step - age_accuracy: 0.5309 - age_loss: 1.2113 - gender_accuracy: 0.8510 - gender_loss: 0.3413 - loss: 2.7777 - val_age_accuracy: 0.5225 - val_age_loss: 1.2008 - val_gender_accuracy: 0.8610 - val_gender_loss: 0.3222 - val_loss: 2.7377 - learning_rate: 1.0000e-05\n",
      "Epoch 3/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 239ms/step - age_accuracy: 0.5701 - age_loss: 1.1107 - gender_accuracy: 0.8731 - gender_loss: 0.3025 - loss: 2.5377"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:35:31.471107: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m138/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:06\u001b[0m 272ms/step - age_accuracy: 0.5678 - age_loss: 1.1151 - gender_accuracy: 0.8703 - gender_loss: 0.3067 - loss: 2.5506"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 240ms/step - age_accuracy: 0.5683 - age_loss: 1.1160 - gender_accuracy: 0.8656 - gender_loss: 0.3135 - loss: 2.5594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m173s\u001b[0m 287ms/step - age_accuracy: 0.5683 - age_loss: 1.1158 - gender_accuracy: 0.8656 - gender_loss: 0.3134 - loss: 2.5588 - val_age_accuracy: 0.5354 - val_age_loss: 1.1539 - val_gender_accuracy: 0.8695 - val_gender_loss: 0.3045 - val_loss: 2.6262 - learning_rate: 1.0000e-05\n",
      "Epoch 4/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 241ms/step - age_accuracy: 0.5856 - age_loss: 1.0608 - gender_accuracy: 0.8840 - gender_loss: 0.2786 - loss: 2.4141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:38:24.665587: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 241ms/step - age_accuracy: 0.5858 - age_loss: 1.0647 - gender_accuracy: 0.8809 - gender_loss: 0.2839 - loss: 2.4272"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 244ms/step - age_accuracy: 0.5907 - age_loss: 1.0632 - gender_accuracy: 0.8759 - gender_loss: 0.2946 - loss: 2.4347"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 294ms/step - age_accuracy: 0.5908 - age_loss: 1.0630 - gender_accuracy: 0.8760 - gender_loss: 0.2945 - loss: 2.4342 - val_age_accuracy: 0.5482 - val_age_loss: 1.1257 - val_gender_accuracy: 0.8753 - val_gender_loss: 0.2948 - val_loss: 2.5602 - learning_rate: 1.0000e-05\n",
      "Epoch 5/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 254ms/step - age_accuracy: 0.6207 - age_loss: 1.0121 - gender_accuracy: 0.8878 - gender_loss: 0.2742 - loss: 2.3122"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:41:23.497020: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 254ms/step - age_accuracy: 0.6179 - age_loss: 1.0159 - gender_accuracy: 0.8855 - gender_loss: 0.2775 - loss: 2.3232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 254ms/step - age_accuracy: 0.6137 - age_loss: 1.0169 - gender_accuracy: 0.8826 - gender_loss: 0.2840 - loss: 2.3316"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 304ms/step - age_accuracy: 0.6136 - age_loss: 1.0167 - gender_accuracy: 0.8827 - gender_loss: 0.2839 - loss: 2.3312 - val_age_accuracy: 0.5507 - val_age_loss: 1.1090 - val_gender_accuracy: 0.8768 - val_gender_loss: 0.2878 - val_loss: 2.5200 - learning_rate: 1.0000e-05\n",
      "Epoch 6/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 262ms/step - age_accuracy: 0.6246 - age_loss: 0.9697 - gender_accuracy: 0.8916 - gender_loss: 0.2598 - loss: 2.2131"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:44:27.472676: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 262ms/step - age_accuracy: 0.6248 - age_loss: 0.9739 - gender_accuracy: 0.8887 - gender_loss: 0.2637 - loss: 2.2254"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 256ms/step - age_accuracy: 0.6264 - age_loss: 0.9768 - gender_accuracy: 0.8880 - gender_loss: 0.2713 - loss: 2.2389"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 307ms/step - age_accuracy: 0.6264 - age_loss: 0.9767 - gender_accuracy: 0.8880 - gender_loss: 0.2713 - loss: 2.2385 - val_age_accuracy: 0.5571 - val_age_loss: 1.0973 - val_gender_accuracy: 0.8791 - val_gender_loss: 0.2832 - val_loss: 2.4919 - learning_rate: 1.0000e-05\n",
      "Epoch 7/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 259ms/step - age_accuracy: 0.6487 - age_loss: 0.9383 - gender_accuracy: 0.8966 - gender_loss: 0.2489 - loss: 2.1393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:47:32.251317: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 258ms/step - age_accuracy: 0.6462 - age_loss: 0.9415 - gender_accuracy: 0.8945 - gender_loss: 0.2526 - loss: 2.1495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 258ms/step - age_accuracy: 0.6430 - age_loss: 0.9417 - gender_accuracy: 0.8930 - gender_loss: 0.2597 - loss: 2.1570"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 308ms/step - age_accuracy: 0.6429 - age_loss: 0.9416 - gender_accuracy: 0.8931 - gender_loss: 0.2597 - loss: 2.1567 - val_age_accuracy: 0.5623 - val_age_loss: 1.0893 - val_gender_accuracy: 0.8824 - val_gender_loss: 0.2790 - val_loss: 2.4716 - learning_rate: 1.0000e-05\n",
      "Epoch 8/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:15\u001b[0m 268ms/step - age_accuracy: 0.6584 - age_loss: 0.9105 - gender_accuracy: 0.9020 - gender_loss: 0.2390 - loss: 2.0740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:50:39.172219: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:04\u001b[0m 268ms/step - age_accuracy: 0.6573 - age_loss: 0.9122 - gender_accuracy: 0.8994 - gender_loss: 0.2427 - loss: 2.0810"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 261ms/step - age_accuracy: 0.6568 - age_loss: 0.9099 - gender_accuracy: 0.8973 - gender_loss: 0.2510 - loss: 2.0848"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 311ms/step - age_accuracy: 0.6569 - age_loss: 0.9098 - gender_accuracy: 0.8973 - gender_loss: 0.2510 - loss: 2.0845 - val_age_accuracy: 0.5623 - val_age_loss: 1.0835 - val_gender_accuracy: 0.8857 - val_gender_loss: 0.2762 - val_loss: 2.4572 - learning_rate: 1.0000e-05\n",
      "Epoch 9/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:04\u001b[0m 248ms/step - age_accuracy: 0.6806 - age_loss: 0.8745 - gender_accuracy: 0.9058 - gender_loss: 0.2346 - loss: 1.9976"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:53:44.502104: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 250ms/step - age_accuracy: 0.6796 - age_loss: 0.8777 - gender_accuracy: 0.9037 - gender_loss: 0.2378 - loss: 2.0071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 256ms/step - age_accuracy: 0.6762 - age_loss: 0.8780 - gender_accuracy: 0.9021 - gender_loss: 0.2439 - loss: 2.0139"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 308ms/step - age_accuracy: 0.6762 - age_loss: 0.8779 - gender_accuracy: 0.9021 - gender_loss: 0.2438 - loss: 2.0136 - val_age_accuracy: 0.5652 - val_age_loss: 1.0799 - val_gender_accuracy: 0.8874 - val_gender_loss: 0.2725 - val_loss: 2.4464 - learning_rate: 1.0000e-05\n",
      "Epoch 10/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 254ms/step - age_accuracy: 0.7008 - age_loss: 0.8461 - gender_accuracy: 0.9034 - gender_loss: 0.2291 - loss: 1.9354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 15:56:50.858321: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 256ms/step - age_accuracy: 0.6987 - age_loss: 0.8487 - gender_accuracy: 0.9018 - gender_loss: 0.2326 - loss: 1.9440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m587/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m6s\u001b[0m 387ms/step - age_accuracy: 0.6924 - age_loss: 0.8490 - gender_accuracy: 0.9035 - gender_loss: 0.2377 - loss: 1.9497"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 426ms/step - age_accuracy: 0.6924 - age_loss: 0.8488 - gender_accuracy: 0.9036 - gender_loss: 0.2377 - loss: 1.9493 - val_age_accuracy: 0.5640 - val_age_loss: 1.0797 - val_gender_accuracy: 0.8878 - val_gender_loss: 0.2698 - val_loss: 2.4433 - learning_rate: 1.0000e-05\n",
      "Epoch 11/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 230ms/step - age_accuracy: 0.7076 - age_loss: 0.8234 - gender_accuracy: 0.9218 - gender_loss: 0.2175 - loss: 1.8784"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 16:01:05.258242: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 239ms/step - age_accuracy: 0.7059 - age_loss: 0.8243 - gender_accuracy: 0.9182 - gender_loss: 0.2208 - loss: 1.8835"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m7s\u001b[0m 432ms/step - age_accuracy: 0.7036 - age_loss: 0.8213 - gender_accuracy: 0.9112 - gender_loss: 0.2285 - loss: 1.8851"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 526ms/step - age_accuracy: 0.7035 - age_loss: 0.8211 - gender_accuracy: 0.9112 - gender_loss: 0.2285 - loss: 1.8848 - val_age_accuracy: 0.5686 - val_age_loss: 1.0768 - val_gender_accuracy: 0.8901 - val_gender_loss: 0.2678 - val_loss: 2.4356 - learning_rate: 1.0000e-05\n",
      "Epoch 12/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:57\u001b[0m 471ms/step - age_accuracy: 0.7247 - age_loss: 0.7938 - gender_accuracy: 0.9171 - gender_loss: 0.2147 - loss: 1.8164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 16:06:46.089567: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:39\u001b[0m 470ms/step - age_accuracy: 0.7233 - age_loss: 0.7950 - gender_accuracy: 0.9156 - gender_loss: 0.2173 - loss: 1.8215"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m7s\u001b[0m 467ms/step - age_accuracy: 0.7171 - age_loss: 0.7938 - gender_accuracy: 0.9129 - gender_loss: 0.2230 - loss: 1.8249"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 558ms/step - age_accuracy: 0.7171 - age_loss: 0.7937 - gender_accuracy: 0.9130 - gender_loss: 0.2230 - loss: 1.8246 - val_age_accuracy: 0.5644 - val_age_loss: 1.0789 - val_gender_accuracy: 0.8926 - val_gender_loss: 0.2650 - val_loss: 2.4370 - learning_rate: 1.0000e-05\n",
      "Epoch 13/15\n",
      "\u001b[1m 96/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:10\u001b[0m 493ms/step - age_accuracy: 0.7356 - age_loss: 0.7671 - gender_accuracy: 0.9274 - gender_loss: 0.2051 - loss: 1.7535"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 16:12:23.734946: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m134/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:51\u001b[0m 494ms/step - age_accuracy: 0.7352 - age_loss: 0.7668 - gender_accuracy: 0.9240 - gender_loss: 0.2084 - loss: 1.7562"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m583/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m9s\u001b[0m 478ms/step - age_accuracy: 0.7323 - age_loss: 0.7638 - gender_accuracy: 0.9181 - gender_loss: 0.2157 - loss: 1.7576 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m343s\u001b[0m 568ms/step - age_accuracy: 0.7323 - age_loss: 0.7637 - gender_accuracy: 0.9182 - gender_loss: 0.2156 - loss: 1.7573 - val_age_accuracy: 0.5671 - val_age_loss: 1.0761 - val_gender_accuracy: 0.8917 - val_gender_loss: 0.2653 - val_loss: 2.4318 - learning_rate: 1.0000e-05\n",
      "Epoch 14/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:51\u001b[0m 460ms/step - age_accuracy: 0.7507 - age_loss: 0.7495 - gender_accuracy: 0.9248 - gender_loss: 0.2004 - loss: 1.7137"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 16:18:04.683864: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:36\u001b[0m 465ms/step - age_accuracy: 0.7482 - age_loss: 0.7493 - gender_accuracy: 0.9223 - gender_loss: 0.2035 - loss: 1.7164"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m8s\u001b[0m 472ms/step - age_accuracy: 0.7425 - age_loss: 0.7426 - gender_accuracy: 0.9192 - gender_loss: 0.2101 - loss: 1.7097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m340s\u001b[0m 563ms/step - age_accuracy: 0.7425 - age_loss: 0.7424 - gender_accuracy: 0.9193 - gender_loss: 0.2101 - loss: 1.7092 - val_age_accuracy: 0.5661 - val_age_loss: 1.0789 - val_gender_accuracy: 0.8926 - val_gender_loss: 0.2639 - val_loss: 2.4361 - learning_rate: 1.0000e-05\n",
      "Epoch 15/15\n",
      "\u001b[1m 99/603\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:11\u001b[0m 499ms/step - age_accuracy: 0.7558 - age_loss: 0.7220 - gender_accuracy: 0.9246 - gender_loss: 0.1985 - loss: 1.6570"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 16:23:48.309985: W tensorflow/core/lib/png/png_io.cc:95] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m137/603\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:49\u001b[0m 492ms/step - age_accuracy: 0.7529 - age_loss: 0.7231 - gender_accuracy: 0.9229 - gender_loss: 0.2011 - loss: 1.6617"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m586/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m8s\u001b[0m 477ms/step - age_accuracy: 0.7497 - age_loss: 0.7170 - gender_accuracy: 0.9213 - gender_loss: 0.2054 - loss: 1.6537"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step - age_accuracy: 0.7497 - age_loss: 0.7168 - gender_accuracy: 0.9214 - gender_loss: 0.2053 - loss: 1.6534\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "\u001b[1m603/603\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 642ms/step - age_accuracy: 0.7497 - age_loss: 0.7168 - gender_accuracy: 0.9214 - gender_loss: 0.2053 - loss: 1.6534 - val_age_accuracy: 0.5652 - val_age_loss: 1.0800 - val_gender_accuracy: 0.8944 - val_gender_loss: 0.2623 - val_loss: 2.4366 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Training: two-phase process with callbacks and fine-tuning\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Callbacks: reduce LR on plateau and early stopping to prevent wasting time\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, min_lr=1e-7, verbose=1)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "# 1) Initial training with frozen base (base model was frozen earlier)\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=[reduce_lr, early_stop]\n",
    ")\n",
    "\n",
    "# 2) Unfreeze the base model for fine-tuning: unfreeze last convolutional blocks\n",
    "base_model.trainable = True\n",
    "# Freeze all layers up to a specific point to avoid catastrophic forgetting\n",
    "fine_tune_at = max(0, len(base_model.layers) - 20)  # unfreeze last ~20 layers (adjust as needed)\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    layer.trainable = (i >= fine_tune_at)\n",
    "\n",
    "# Re-compile with a much lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss={\"age\": keras.losses.SparseCategoricalCrossentropy(), \"gender\": keras.losses.SparseCategoricalCrossentropy()},\n",
    "    loss_weights={\"age\": 2.0, \"gender\": 1.0},\n",
    "    metrics={\"age\": \"accuracy\", \"gender\": \"accuracy\"}\n",
    ")\n",
    "\n",
    "# Continue training (fine-tuning)\n",
    "history_finetune = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[reduce_lr, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e32d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m151/151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 378ms/step\n",
      "Age Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Bin 0       0.85      0.86      0.86       667\n",
      "       Bin 1       0.59      0.40      0.47       311\n",
      "       Bin 2       0.60      0.76      0.67      1473\n",
      "       Bin 3       0.38      0.27      0.32       909\n",
      "       Bin 4       0.28      0.22      0.25       450\n",
      "       Bin 5       0.38      0.39      0.38       462\n",
      "       Bin 6       0.66      0.72      0.69       549\n",
      "\n",
      "    accuracy                           0.57      4821\n",
      "   macro avg       0.53      0.52      0.52      4821\n",
      "weighted avg       0.55      0.57      0.55      4821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 8.5 — Evaluate Age Performance Per Bin\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get predictions on validation set\n",
    "val_preds = model.predict(val_ds)\n",
    "age_preds = val_preds[0].argmax(axis=1)  # age predictions\n",
    "age_true = valA['age_bin'].values\n",
    "\n",
    "print(\"Age Classification Report:\")\n",
    "print(classification_report(age_true, age_preds, target_names=[f'Bin {i}' for i in range(num_age)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e66a51",
   "metadata": {},
   "source": [
    "### STEP 9 — Predict Pseudo-Labels for Dataset B\n",
    "\n",
    "Create loader for B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d1d4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate pseudo-labels for dataset B using the trained model\n",
    "\n",
    "IMG_SIZE = 224  # must match model input size\n",
    "\n",
    "# Function to load and preprocess images for dataset B\n",
    "def load_image(path):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "    img = keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create a TensorFlow dataset for dataset B\n",
    "def create_datasetB(df):\n",
    "    image_paths = df[\"image_path\"].values\n",
    "    \n",
    "    # Create a TensorFlow dataset from the image paths\n",
    "    ds = tf.data.Dataset.from_tensor_slices((image_paths))\n",
    "\n",
    "    # Map the dataset to load and preprocess images\n",
    "    def process(path) :\n",
    "        img = load_image(path)\n",
    "        return img, path\n",
    "    \n",
    "    # Map the dataset to load and preprocess images and return labels\n",
    "    ds = ds.map(process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE) # Prefetch for performance\n",
    "    return ds\n",
    "\n",
    "\n",
    "dsB = create_datasetB(dfB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78b74ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 16:32:53.184117: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Generate pseudo-labels predictions for dataset B using the trained model\n",
    "\n",
    "# Store predictions and confidence scores\n",
    "age_preds = []\n",
    "gender_preds = []\n",
    "\n",
    "age_conf = []\n",
    "gender_conf = []\n",
    "\n",
    "# Iterate through dataset B and get predictions from the model\n",
    "for images, paths in dsB:\n",
    "    age_p, gender_p = model.predict(images, verbose=0)\n",
    "    \n",
    "    age_preds.extend(np.argmax(age_p, axis=1))\n",
    "    gender_preds.extend(np.argmax(gender_p, axis=1))\n",
    "    \n",
    "    age_conf.extend(np.max(age_p, axis=1))\n",
    "    gender_conf.extend(np.max(gender_p, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42988332",
   "metadata": {},
   "source": [
    "### STEP 10 — Create Augmented Dataset B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b1e88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          image_path  emotion  age_pseudo  \\\n",
      "0  source_data/raf/DATASET/train/7/train_11651_al...        7           2   \n",
      "1  source_data/raf/DATASET/train/7/train_10043_al...        7           0   \n",
      "2  source_data/raf/DATASET/train/7/train_11301_al...        7           2   \n",
      "3  source_data/raf/DATASET/train/7/train_10513_al...        7           2   \n",
      "4  source_data/raf/DATASET/train/7/train_11148_al...        7           2   \n",
      "\n",
      "   gender_pseudo  age_conf  gender_conf  \n",
      "0              0  0.360979     0.501508  \n",
      "1              1  0.671543     0.961124  \n",
      "2              1  0.919729     0.892669  \n",
      "3              1  0.489154     0.758168  \n",
      "4              0  0.736868     0.571540  \n"
     ]
    }
   ],
   "source": [
    "# Add the pseudo-labels and confidence scores to the original dataframe for dataset B\n",
    "dfB[\"age_pseudo\"] = age_preds\n",
    "dfB[\"gender_pseudo\"] = gender_preds\n",
    "\n",
    "dfB[\"age_conf\"] = age_conf\n",
    "dfB[\"gender_conf\"] = gender_conf\n",
    "\n",
    "print(dfB.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c9a86",
   "metadata": {},
   "source": [
    "### STEP 11 — Confidence Filtering\n",
    "\n",
    "We don’t trust low-confidence predictions. Therefore it is necessary for us to drop those predictions\n",
    "\n",
    "* -1 = unknown\n",
    "* Others = pseudo-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "343f074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.50 # Set a confidence threshold for accepting pseudo-labels\n",
    "\n",
    "# Set pseudo-labels to -1 for samples where confidence is below the threshold\n",
    "dfB.loc[dfB[\"age_conf\"] < THRESHOLD, \"age_pseudo\"] = -1\n",
    "dfB.loc[dfB[\"gender_conf\"] < THRESHOLD, \"gender_pseudo\"] = -1\n",
    "\n",
    "# Save the updated dataframe with pseudo-labels to a new CSV file`\n",
    "dfB.to_csv(\"B_with_pseudo_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c04ba86",
   "metadata": {},
   "source": [
    "### STEP 12 — Merge Datasets\n",
    "\n",
    "Keep true vs pseudo separate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f370f6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_path', 'age', 'gender', 'Race', 'age_bin'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dfA.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32f3f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets A and B using the true labels from dataset A and the pseudo-labels from dataset B.\n",
    "\n",
    "dfA[\"emotion\"] = -1  # no emotion label in A\n",
    "\n",
    "dfA[\"age\"] = dfA[\"age_bin\"]        # true age\n",
    "dfA[\"gender\"] = dfA[\"gender\"]  # true gender\n",
    "\n",
    "dfA = dfA[[\"image_path\", \"age\", \"gender\", \"emotion\"]]\n",
    "\n",
    "\n",
    "# Since dataset B does not have true labels, we will use the pseudo-labels as the \"true\" labels for merging. \n",
    "# We will also keep the original columns for clarity, but they will be filled with NaN since we don't have true labels for dataset B.\n",
    "dfB[\"age\"] = dfB[\"age_pseudo\"]\n",
    "dfB[\"gender\"] = dfB[\"gender_pseudo\"]\n",
    "dfB[\"emotion\"] = dfB[\"emotion\"]\n",
    "\n",
    "dfB = dfB[[\"image_path\", \"age\", \"gender\", \"emotion\"]]\n",
    "\n",
    "# Merge the two datasets\n",
    "merged = pd.concat([dfA, dfB], ignore_index=True)\n",
    "merged.to_csv(\"merged_dataset.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning (3.9.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
